<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Paul A. Bloom" />


<title>Into the (Bayesian) Multiverse!</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Amygdala Multiverse</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Info</a>
</li>
<li>
  <a href="into_the_bayesian_multiverse.html">Multiverse Code Walkthrough</a>
</li>
<li>
  <a href="https://github.com/pab2163/amygdala_mpfc_multiverse">Project Analysis Code &amp; Docs</a>
</li>
<li>
  <a href="https://pbloom.shinyapps.io/amygdala_mpfc_multiverse/">Interactive Multiverse Explorer</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Into the (Bayesian) Multiverse!</h1>
<h4 class="author">Paul A. Bloom</h4>

</div>


<div id="background" class="section level1">
<h1>0. Background</h1>
<p>Welcome! This document serves 2 purposes:</p>
<ol style="list-style-type: decimal">
<li>Provide a walkthough for anyone potentially interested in conducting multiverse analyses (or specification curves), especially when considering Bayesian models or fMRI data.</li>
<li>Serve as documentation for analysis code for <a href="https://www.biorxiv.org/content/10.1101/2021.10.08.463601v1">Bloom et al., 2021</a> with code that can be run using <em>simulated data</em> (i.e. fake data), since we cannot share the data publicly. Note: the simulated data &amp; corresponding code walkthrough are specifically for amygdala reactivity analyses for the fear &gt; baseline contrast.</li>
</ol>
<p>Want to try the code out yourself? You can clone the <a href="https://github.com/pab2163/amygdala_mpfc_multiverse">Github repo</a>, or download the <a href="https://github.com/pab2163/amygdala_mpfc_multiverse/blob/master/docs/simulated_amygdala_reactivity.csv">simulated data</a> and the <a href="https://github.com/pab2163/amygdala_mpfc_multiverse/blob/master/9_specification_curve_walkthrough/into_the_bayesian_multiverse.Rmd">into_the_bayesian_multiverse.Rmd</a> file to run &amp; experiment with the code.</p>
<div id="what-is-a-multiverse-anyway-what-is-a-specification-curve" class="section level2">
<h2>What is a multiverse anyway? What is a specification curve?</h2>
<p>A <strong>multiverse analysis</strong> is an analytical tool that helps provide an understanding of whether study results hinge on decisions made during the analysis process. Briefly, a multiverse analysis means that one identifies a set of analyses methods that are all theoretically justified <em>then conducts all of these analysis “specifications” in parallel</em>. Then, a <strong>specification curve</strong> can be used for visualization of all of the different analysis specifications at once, as well as statistical inference.</p>
<p>The specification curve analyses here were particularly inspired by work from <a href="https://dcosme.github.io/specification-curves/SCA_tutorial_inferential">Dani Cosme</a> and <a href="https://www.amyorben.com/pdf/2019_orbenprzybylski_nhb.pdf">Amy Orben</a>.</p>
<p>See more background on the statistics <a href="https://www.nature.com/articles/s41562-020-0912-z">here</a> and <a href="https://journals.sagepub.com/doi/10.1177/1745691616658637">here</a>.</p>
<div id="a-fun-way-to-think-about-multiverse-analyses" class="section level3">
<h3>A fun way to think about multiverse analyses…</h3>
<p>If you’ve ever seen <a href="https://www.imdb.com/title/tt4633694/">Spider Man: Into The Spiderverse</a>, first of all – its a great movie. But more specifically, there are many “parallel universes” with different versions of “spider people” in each one. The “spidey-essence” is the same in all universes, but each spider-person (or pig…) is a little different based on the specifics of that universe. So, we can think of the raw data as the “spidey-essence” in a multiverse analysis, and each model specification is a different “parallel universe” of how the results of the analysis could come out when combined with a certain set of analytical decisions.</p>
<p><img src="images/spideyverse.png" /><br />
I’m not an expert, but “the multiverse” is now part of the canon of the <a href="https://marvel.fandom.com/wiki/Multiverse">Marvel Cinematic Universe</a>…</p>
</div>
</div>
<div id="in-what-situation-should-i-run-a-multiverse-analysis" class="section level2">
<h2>In what situation should I run a multiverse analysis?</h2>
<p>Multiverse analyses are particularly useful when there are <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf">decisions we have to make</a> during the data preparation and analysis process where we aren’t sure what the “right” answer is. If we can think of different methods for cleaning, preprocessing, excluding, transforming, or modeling the data that are theoretically justifiable (but we don’t have a certain answer for which is ‘best’) multiverse analysis allows us to run all of these theoretically justifiable analyses at the same time. This allows us to look at <strong>2 main questions</strong>:</p>
<ol style="list-style-type: decimal">
<li>Do the results from differing, theoretically justifiable, versions of the analyses converge on a consistent finding?</li>
<li>How do different choices we make as analysts of the data influence the results? Which decisions are most influential?</li>
</ol>
<p>These analyses have been particularly useful with neuroimaging data where there are MANY decisions to be made where we don’t have consensus for an “optimal” method. However, multiverse analyses and specification curves equally useful in any topic of research where there are multiple theoretically justifiable ways an analysis can be done.</p>
</div>
<div id="specification-curve-analysis-steps" class="section level2">
<h2>Specification curve analysis steps</h2>
<p><a href="https://www.nature.com/articles/s41562-020-0912-z">Simonsohn et al (2020)</a> describe 3 steps for specification curve analyses:</p>
<ol style="list-style-type: decimal">
<li>define the set of reasonable specifications to estimate</li>
<li>estimate all specifications and report the results in a descriptive specification curve</li>
<li>conduct joint statistical tests using an inferential specification curve.</li>
</ol>
<p>Because the analyses presented here are considered exploratory, we aren’t going to cover the inferential procedures in step #3. For more info on this step, including applying bootstrapping &amp; permutation testing to spec curves, see this <a href="https://dcosme.github.io/specification-curves/SCA_tutorial_inferential#4_Inferential_statistics">great tutorial from Dani Cosme</a>.</p>
</div>
<div id="note-simulated-data-here-only" class="section level2">
<h2>Note: simulated data here only!</h2>
<p><strong>Note:</strong> these data are fake! Under our Institutional Review Board protocol and for the purpose of creating participant identities private, we cannot share our actual data publicly. However, we’ve <a href="https://github.com/pab2163/amygdala_mpfc_multiverse/blob/master/7_group_level_analyses/simulate_amygdala_reactivity/simulate_reactivity_data.Rmd"><em>simulated</em></a> an amygdala reactivity dataset for the purposes of creating a multiverse analysis where the code can actually be run! You can find the simulated data <a href="https://github.com/pab2163/amygdala_mpfc_multiverse/blob/master/docs/simulated_amygdala_reactivity.csv">here</a></p>
<ul>
<li>Briefly, to create simulated data that <em>decently</em> approximates the real data without risk of identifying participants, we created a multivariate regression model using <a href="https://github.com/paul-buerkner/brms">brms</a>, then drew samples from the model’s posterior predictive distribution for fear &gt; baseline amygdala reactivity estimates for each study time point for each real participant.</li>
<li>We also scrambled the order of participant IDs and added noise to ages.</li>
<li>So, the data here should do a <em>reasonable</em> job of mimicking the <em>structure</em> of some of the data analyzed in <a href="https://www.biorxiv.org/content/10.1101/2021.10.08.463601v1">Bloom et al. (2021)</a> without compromising participant data privacy and security. Some of the relationships among variables may differ somewhat from the real data, however. For more on data synthesis for these purposes, check out the <a href="https://cran.r-project.org/web/packages/synthpop/vignettes/synthpop.pdf">synthpop</a> R package.</li>
</ul>
</div>
</div>
<div id="read-in-the-data" class="section level1">
<h1>1. Read in the data</h1>
<pre class="r"><code>library(tidyverse)
library(specr)
library(brms)
library(cowplot)</code></pre>
<p>Here’s what is in each column:</p>
<ul>
<li><code>id</code> - participant ID, identifies a participant across time points</li>
<li><code>wave</code> - the study time point (either <code>1</code>, <code>2</code>, or <code>3</code>)</li>
<li><code>age</code> - participant age at the given time point, in years</li>
<li><code>block</code> - the temporal position of the task run relative to other tasks in the scanner (<code>1</code> = first, <code>2</code> = second, <code>3</code> = third)</li>
<li><code>motion</code> - head motion (mean framewise displacement), which has been z-scored here</li>
<li><code>scanner</code> - whether the data were collected on a first MRI scanner (<code>1</code>= time points 1 &amp; 2) or a second (<code>2</code> = time point 3). Both were Siemens Tim Trio</li>
<li><code>prev_studied</code>- whether this scan was previously analyzed in similar work by <a href="https://www.jneurosci.org/content/33/10/4584">Gee et al (2013)</a>. <code>1</code> indicates a scan was previously studied</li>
</ul>
<p>All of the rest of the columns are measurements of amygdala reactivity to fear faces &gt; baseline for each scan, labeled such that:</p>
<ul>
<li>columns with the <code>ho</code> prefix are from amygdala ROIs defined by the Harvard-Oxford subcortical atlas, <code>native</code> prefix columns are in native space defined through Freesurfer</li>
<li>columns with the <code>right</code> prefix are the right amygdala, and <code>left</code> are the left</li>
<li>columns with the <code>beta</code> prefix denote raw beta estimates of amygdala reactivity magnitude, while the <code>tstat</code> prefix denote t-statistic measurements of amygdala reactivity scaled by estimation uncertainty (the standard error)</li>
</ul>
<pre class="r"><code>fake_data = readr::read_csv(&#39;simulated_amygdala_reactivity.csv&#39;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   id = col_double(),
##   wave = col_double(),
##   age = col_double(),
##   block = col_double(),
##   motion = col_double(),
##   scanner = col_double(),
##   prev_studied = col_double(),
##   ho_right_amyg_beta = col_double(),
##   ho_left_amyg_beta = col_double(),
##   ho_right_amyg_tstat = col_double(),
##   ho_left_amyg_tstat = col_double(),
##   native_right_amyg_beta = col_double(),
##   native_left_amyg_beta = col_double(),
##   native_right_amyg_tstat = col_double(),
##   native_left_amyg_tstat = col_double()
## )</code></pre>
<pre class="r"><code>head(fake_data[,1:8])</code></pre>
<pre><code>## # A tibble: 6 x 8
##      id  wave   age block motion scanner prev_studied ho_right_amyg_beta
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;              &lt;dbl&gt;
## 1     1     3 16.1      1  0.293       2           NA            0.785  
## 2     2     1 15.2      3  0.08        1            1            0.375  
## 3     2     2 17.3      1 -1.32        1           NA           -0.168  
## 4     2     3 19.4      1  0.063       2           NA           -0.230  
## 5     3     1 22.7      2 -0.986       1           NA            0.00908
## 6     7     1  5.83     1  1.18        1           NA           -0.817</code></pre>
<div id="a.-accelerated-longitudinal-study-design" class="section level2">
<h2>1A. Accelerated Longitudinal Study Design</h2>
<p>Here’s a plot displaying when each (fake) participant came in for visits. The x-axis indicates the age of the participants at each visit, and participants (on the y-axis) are ordered by the age at their first study visit. Horizontal lines connecting dots indicate the multiple study visits over time for the same participant (although some participants only completed 1 visit)</p>
<pre class="r"><code>designPlot = dplyr::mutate(fake_data, wave = as.factor(wave)) %&gt;%
  ggplot(aes(x = age, y = forcats::fct_reorder(as.factor(id), age))) +
  geom_line(aes(group = id)) + 
  geom_point(aes(color = wave)) +
  theme_bw() + theme(axis.text.y = element_blank(), panel.grid.major = element_blank()) +
  labs(y = &#39;Participants&#39;, x = &#39;Age at visit&#39;) +
  guides(color = guide_legend(title = &#39;Study Wave&#39;)) +
  theme(text = element_text(face = &#39;bold&#39;),
        axis.ticks.y=element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

designPlot</code></pre>
<p><img src="into_the_bayesian_multiverse_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
</div>
<div id="specification-curve-specr-version" class="section level1">
<h1>2. Specification curve: <code>specr</code> version</h1>
<p><a href="https://github.com/masurp/specr"><img src="images/specr_logo.png" alt="specr" style="width:15.0%" /></a></p>
<p><a href="https://cran.r-project.org/web/packages/specr/vignettes/specr.html">specr</a> is a great package with a lot of nice functionality for running specification curve analyses. This wasn’t the software we used for the main analyses (because it hadn’t yet been released yet), but it’s probably the most straightforward way to go for regression-based specification curve analyses.</p>
<div id="a.-define-all-reasonable-specifications" class="section level2">
<h2>2A. Define all ‘reasonable’ specifications</h2>
<p>First, we’ll need to define what different ‘reasonable’ and theoretically justifiable ways there are for doing this analysis of age-related change in amygdala reactivity to fear faces &gt; baseline. For the sake of this project, it isn’t computationally feasible to try ALL reasonable ways of doing this analysis, so we can think of this approach as attempting to sample from the space of possible analyses specifications.</p>
<ul>
<li><strong>Covariates:</strong> it might make sense to adjust for factors like <code>scanner</code> and <code>block</code> in our group-level model, or different subsets of these.</li>
<li><strong>Amygdala ROI:</strong> there are several justifiable ways of defining an ‘amygdala’ brain region, either using a “native space” mask generated through Freesurfer, or by using Harvard-Oxford amygdala estimates in standard space. We could also us right, left, or bilateral amygdala estimates</li>
<li><strong>Estimate type:</strong> Here we pull both from the <code>beta</code> images and <code>tstat</code> images for 2 different amygdala reactivity estimates. The <code>beta</code> is the <em>contrast parameter estimate</em> and represents the point estimate (beta estimate) of the regression coefficient. Thus, the <code>beta</code> is the <em>magnitude</em> of the estimated relationship between the presence of the task stimuli and the BOLD signal. The <code>tstat</code> represents the estimate scaled by the uncertainty (i.e. by the standard error of the estimate), and so it is a <em>standardized</em> effect size measure of the relationship between the presence of the task stimuli and the BOLD signal</li>
<li><strong>Data inclusion:</strong> Some of the scans studied here were previously analyzed for age-related change in amygdala reactivity by <a href="https://www.jneurosci.org/content/33/10/4584">Gee et al (2013).</a>. So, to get an estimate more independent from the previous work, we might want to exclude these. However, we might have higher statistical power if we include all the available data.</li>
</ul>
<p>First, a little data wrangling before modeling.</p>
<ul>
<li>We’ll create a variable called <code>grp</code> to index whether scans were previously studied for age-related changes in amygdala reactivity in <a href="https://www.jneurosci.org/content/33/10/4584">Gee et al. (2013)</a>. The <code>not_studied</code> group here can be considered a more independent sample from the previous study.</li>
<li>We’ll also create a vector of the outcomes (different measures of amygdala reactivity)</li>
</ul>
<pre class="r"><code>fake_data = dplyr::mutate(fake_data, grp = ifelse(is.na(prev_studied), &#39;not_studied&#39;, &#39;prev_studied&#39;))
outcomes = names(fake_data)[grepl(&#39;amyg&#39;, names(fake_data))]</code></pre>
</div>
<div id="b.-specify-the-custom-model" class="section level2">
<h2>2B. Specify the custom model</h2>
<p><code>Specr</code> has a nice feature of allowing a custom model function. For more detailed info on this, see <a href="https://masurp.github.io/specr/articles/random_effects.html">here</a>.</p>
<p>Here, we set up a multilevel model using the <code>lme4</code> package’s <code>lmer()</code> function to take advantage of varying intercepts for each participant (as denoted by the <code>(1|id)</code> syntax in the model formula). We also include a covariate for <code>motion</code> in <em>the custom formula itself</em> (rather than later on in the spec curve), because we want <em>all</em> models to include this covariate for head motion during the scan. We also need to load the <code>lme4</code> and <code>broom.mixed</code> packages inside the custom model function so that <code>specr</code> can use them to run the model and parse the results.</p>
<pre class="r"><code>lmer_model_with_motion &lt;- function(formula, data,...) {
  require(lme4)
  require(broom.mixed)
  # set up the model base formula (basically specr will past all other model info in here)
  formula &lt;- paste(formula, &quot;+ motion + (1|id)&quot;)
  lme4::lmer(formula, data)
}</code></pre>
</div>
<div id="c.-run-all-model-specifications-with-run_specs" class="section level2">
<h2>2C. Run all model specifications with <code>run_specs()</code></h2>
<p>How does this <a href="https://masurp.github.io/specr/reference/run_specs.html"><code>run_specs()</code></a> function work?</p>
<ul>
<li><code>df=fake_data</code>: specifies the data frame to fit the model to</li>
<li><code>x = 'age'</code>: the ‘x’ variable, or predictor of interest in the model. You can have multiple x variables if you want by specifying a vector with multiple values here</li>
<li><code>y = outcomes</code>: here we specify a vector of multiple outcome variables, the 8 different measures of amygdala reactivity</li>
<li><code>controls = c('block', 'scanner')</code>: covariates to include in the model.</li>
<li><code>model = 'lmer_model_with_motion'</code>: specifies the custom model. If you don’t need a custom model, there are also defaults, like <code>&quot;lm&quot;</code> to run OLS regression</li>
<li><code>subsets = list(grp = 'not_studied')</code>: subsets of the data to run the model on, expressed as a list. Here, we want to run the model specifically on the <code>not_studied</code> group, in addition to the full dataset.</li>
<li>Because we have <code>all.comb=TRUE</code>, this will run specifications with all possible combinations of covariates. Otherwise, it will run models with no covariates, each single covariate, and then all together. Note: random slopes are not included for these covariates by default unless we add this to the custom model</li>
</ul>
<p>Here, 8 outcomes X 4 possible covariate specifications X 2 subsets = 64 total specifications in our multiverse</p>
<pre class="r"><code>specs = specr::run_specs(df = fake_data,
                         x = &#39;age&#39;, y = outcomes, 
                         controls = c(&#39;block&#39;, &#39;scanner&#39;),
                         model = &#39;lmer_model_with_motion&#39;,
                         subsets = list(grp = &#39;not_studied&#39;), 
                         all.comb = TRUE)</code></pre>
</div>
<div id="d.-check-out-results-summaries-across-all-models" class="section level2">
<h2>2D. Check out results summaries across all models</h2>
<p>The output of <code>run_specs()</code> is a data frame with each row representing one specification from the multiverse analysis we have just run. The <code>x</code>, <code>y</code>, <code>model</code>, <code>controls</code> and <code>subsets</code> columns give us information on the setup of each model, then we get a variety of statistical outputs from the fit models. Because we have 64 specifications here, the dataframe is 64 rows.</p>
<ul>
<li><code>estimate</code>: the beta estimate in the regression model for the given <code>x</code> term of interest (or the ‘slope’ for a continuous predictor). Here, these beta estimates represent estimates for age-related change in amygdala reactivity, such that negative estimates indicate age-related decreases in amygdala reactivity.</li>
<li><code>std.error</code>: The standard error about the beta estimate</li>
<li><code>statistic</code>: The t-statistic for the beta estimate</li>
<li><code>conf.low</code>: The lower bound of the 95% confidence interval for the beta estimate</li>
<li><code>conf.high</code>: The upper bound of the 95% confidence interval for the beta estimate</li>
<li>all of the <code>fit_</code> columns provide information on the goodness-of-fit of the model more generally (i.e. log likelihood, AIC, BIC), as well as the residual degrees of freedom.</li>
</ul>
<pre class="r"><code>specs</code></pre>
<pre><code>## # A tibble: 64 x 18
##    x     y     model controls effect group estimate std.error statistic conf.low
##    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
##  1 age   ho_r… lmer… block    fixed  &lt;NA&gt;   -0.0416    0.0145     -2.87  -0.0700
##  2 age   ho_l… lmer… block    fixed  &lt;NA&gt;   -0.0390    0.0146     -2.67  -0.0677
##  3 age   ho_r… lmer… block    fixed  &lt;NA&gt;   -0.0423    0.0142     -2.97  -0.0702
##  4 age   ho_l… lmer… block    fixed  &lt;NA&gt;   -0.0298    0.0135     -2.21  -0.0562
##  5 age   nati… lmer… block    fixed  &lt;NA&gt;   -0.0313    0.0159     -1.97  -0.0624
##  6 age   nati… lmer… block    fixed  &lt;NA&gt;   -0.0303    0.0156     -1.94  -0.0609
##  7 age   nati… lmer… block    fixed  &lt;NA&gt;   -0.0318    0.0157     -2.03  -0.0624
##  8 age   nati… lmer… block    fixed  &lt;NA&gt;   -0.0269    0.0142     -1.89  -0.0548
##  9 age   ho_r… lmer… scanner  fixed  &lt;NA&gt;   -0.0405    0.0146     -2.78  -0.0690
## 10 age   ho_l… lmer… scanner  fixed  &lt;NA&gt;   -0.0386    0.0147     -2.63  -0.0674
## # … with 54 more rows, and 8 more variables: conf.high &lt;dbl&gt;, fit_sigma &lt;dbl&gt;,
## #   fit_logLik &lt;dbl&gt;, fit_AIC &lt;dbl&gt;, fit_BIC &lt;dbl&gt;, fit_REMLcrit &lt;dbl&gt;,
## #   fit_df.residual &lt;int&gt;, subsets &lt;chr&gt;</code></pre>
</div>
<div id="e.-plot-the-specification-curve" class="section level2">
<h2>2E. Plot the specification curve</h2>
<pre class="r"><code>specr::plot_specs(specs)</code></pre>
<p><img src="into_the_bayesian_multiverse_files/figure-html/unnamed-chunk-8-1.png" width="768" /></p>
<p>This type of plot can be overwhelming at first because it shows us a <strong>lot</strong> of information, but it is really helpful for looking at several different things!</p>
<ul>
<li><strong>Panel A</strong> shows us the regression beta estimates for our <code>x</code> predictor (plus 95% confidence intervals) for all of our specifications. Specifications are ordered from least to greatest, and colored such that red indicates a significant negative association, gray indicates a non-significant association, and blue (not shown on this plot) indicates a significant positive association. So, here, the majority of the specifications indicate a significant negative estimate (under <span class="math inline">\(\alpha=0.05\)</span>, such that the 95% confidence interval excludes 0), suggesting a negative relationship here between age and amygdala reactivity. Here, we can see that the <em>point estimates</em> of all the models are negative, indicating that all the models agreed on the <em>direction</em> of age-related change, if not the significance.</li>
<li><strong>Panel B</strong> shows us the details for each specification, keeping the coloring schema from Panel A based on estimate direction/significance. So, looking vertically across the both panels tells us specifics about a single model specification. On any given line, a dash (<code>|</code>) indicates that the choice marked on the left was made, where a blank space it wasn’t made.
<ul>
<li>The top section <code>age</code> indicates that that the <code>x</code> variable (the term being examined) in all models was <code>age</code>.</li>
<li>In the <code>y</code> section below that, dashes on a given line indicate what <code>y</code> variable for amygdala reactivity was used in a given specification (note that different choices for <code>y</code> are mutually exclusive).</li>
<li>The <code>model</code> section indicates that the <code>lmer_model_with_motion</code> was the model used in all specifications here.</li>
<li>The <code>controls</code> section shows which covariates were used in each specification, although we should remember here that ALL models had a covariate for head motion built into them (so <code>no covariates</code> really means no covariates <em>other than head motion</em>)</li>
<li>The bottom section <code>subsets</code> indicates which subset of the data the model was run on, either <code>all</code>, or just the <code>not_studied</code> group.</li>
</ul></li>
<li>So, what is <strong>Panel B</strong> telling us practically? We can see how estimates term of interest differ as a function of different model specification decision points! In general, if specifications tend to show up towards the left side of the graph, this means that the specifications resulted in <em>more negative</em> beta estimates (more negative age-related change in amygdala reactivity here), while specifications that tend to show up towards the right side of the graph tended to result in <em>more positive</em> beta estimates. Most specifically, we can see that here in the simulated data:
<ul>
<li>Specifications with <code>ho_right_amyg_tstat</code> or <code>ho_right_amyg_beta</code> as the <code>y</code> variable tended to result in the most strongly negative estimates compared to the others (they show up towards the left side)</li>
<li>Specifications with <code>native_left_amyg_tstat</code> as the <code>y</code> variable tended to result in the most strongly positive estimates compared to the others (they show up towards the right side)</li>
</ul></li>
</ul>
</div>
<div id="f.-view-the-multiverse-decision-tree" class="section level2">
<h2>2F. View the multiverse decision tree</h2>
<p><code>specr</code> also gives us a nice <code>plot_decisiontree()</code> function for summarizing what our specifications are, and how many of them there are</p>
<pre class="r"><code>specr::plot_decisiontree(specs, legend = TRUE)</code></pre>
<p><img src="into_the_bayesian_multiverse_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
</div>
<div id="a-bayesian-specification-curve-with-specr-and-brms" class="section level1">
<h1>3. A bayesian specification curve with <code>specr</code> and <code>brms</code></h1>
<p>For many analyses, we might want to use Bayesian estimation for models in our specification curve, particularly if we are getting convergence issues or singular fits with <code>lmer</code>. In this cases, <code>lmer</code> will refuse to let us fit a fully specified longitudinal model with varying terms (slopes) for age for each participant because of the high number of model parameters relative to data points. However, the <a href="https://cran.r-project.org/web/packages/brms/index.html">brms</a> R package can fit these models with nearly identical syntax to <code>lmer</code> using <a href="https://mc-stan.org/">Stan</a> for fully Bayesian estimation under the hood (note: the high number of parameters relative to the number of observations is <em>definitely still a problem</em>, but with Bayesian estimation we can at least fit the model and quantify uncertainty for each parameter).</p>
<p>While they have many advantages, the main drawback to Bayesian models it that they take longer to fit and require more RAM than do frequentist models.</p>
<ul>
<li>When fitting these yourself, you may want to plan to run 1 model first to get an approximate timing estimate <em>before</em> you run a Bayesian specification curve.</li>
<li>If available, servers or computing clusters (especially if you can parallelize across specifications) can vastly speed up Bayesian specification curve analyses.</li>
</ul>
<div id="a.-define-a-custom-brms-model-with-participant-varying-age-terms" class="section level2">
<h2>3A. Define a custom <code>brms</code> model with participant-varying age terms</h2>
<p>The <code>brm()</code> function from the <a href="https://cran.r-project.org/web/packages/brms/index.html">brms</a> package fits models using Bayesian estimation using mostly the same syntax as <code>lme4</code> models. Here, we set up a custom model function, making sure to include <code>(age|id)</code> to allow age terms to vary for each participant. Also, inside the call to fit the model, we’ll add <code>cores=4</code> to parallelize running of each model across 4 cores. Most laptops have 4-8 cores, so this should work on most computers, and should speed up each model by a factor of about 4.</p>
<pre class="r"><code>brms_model_with_motion &lt;- function(formula, data,...) {
  require(brms)
  require(broom.mixed)
  # set up the model base formula (basically specr will paste all other model info in here)
  formula &lt;- paste(formula, &quot;+ motion + (age|id)&quot;)
  brms::brm(formula, data, cores = 4)
}</code></pre>
</div>
<div id="b.-run-all-bayesian-model-specifications-with-run_specs" class="section level2">
<h2>3B. Run all Bayesian model specifications with <code>run_specs()</code></h2>
<p>We’ll run the equivalent specifications to the previous curve, only now with <code>model = 'brms_model_with_motion'</code> for our custom <code>brms</code> model</p>
<pre class="r"><code>brms_specs = specr::run_specs(df = fake_data,
                         x = &#39;age&#39;, y = outcomes, 
                         controls = c(&#39;block&#39;, &#39;scanner&#39;),
                         model = &#39;brms_model_with_motion&#39;,
                         subsets = list(grp = &#39;not_studied&#39;), 
                         all.comb = TRUE)</code></pre>
</div>
<div id="c.-check-out-bayesian-model-summaries" class="section level2">
<h2>3C. Check out Bayesian model summaries</h2>
<p>The summary here is quite similar to when we used the <code>lme4</code> model, only now the <code>estimate</code> is the median estimate for the <code>x</code> term (age) across posterior samples, and <code>conf.low</code> and <code>conf.high</code> represent the bounds of the 95% posterior interval.</p>
<pre class="r"><code>brms_specs</code></pre>
<pre><code>## # A tibble: 64 x 15
##    x     y     model controls effect component group estimate std.error conf.low
##    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
##  1 age   ho_r… brms… block    fixed  cond      &lt;NA&gt;   -0.0386    0.0156  -0.0681
##  2 age   ho_l… brms… block    fixed  cond      &lt;NA&gt;   -0.0383    0.0154  -0.0677
##  3 age   ho_r… brms… block    fixed  cond      &lt;NA&gt;   -0.0405    0.0152  -0.0704
##  4 age   ho_l… brms… block    fixed  cond      &lt;NA&gt;   -0.0287    0.0147  -0.0568
##  5 age   nati… brms… block    fixed  cond      &lt;NA&gt;   -0.0264    0.0175  -0.0601
##  6 age   nati… brms… block    fixed  cond      &lt;NA&gt;   -0.0284    0.0165  -0.0601
##  7 age   nati… brms… block    fixed  cond      &lt;NA&gt;   -0.0294    0.0171  -0.0622
##  8 age   nati… brms… block    fixed  cond      &lt;NA&gt;   -0.0261    0.0153  -0.0563
##  9 age   ho_r… brms… scanner  fixed  cond      &lt;NA&gt;   -0.0385    0.0156  -0.0696
## 10 age   ho_l… brms… scanner  fixed  cond      &lt;NA&gt;   -0.0374    0.0157  -0.0677
## # … with 54 more rows, and 5 more variables: conf.high &lt;dbl&gt;,
## #   fit_algorithm &lt;chr&gt;, fit_pss &lt;dbl&gt;, fit_nobs &lt;int&gt;, subsets &lt;chr&gt;</code></pre>
</div>
<div id="d.-plot-bayesian-specification-curve" class="section level2">
<h2>3D. Plot Bayesian specification curve</h2>
<p>With the Bayesian models including participant-varying age terms, results are largely similar to before. The main difference is that a slightly larger proportion of the 95% posterior intervals overlap with 0 than with the previous models without participant-varying age terms.</p>
<pre class="r"><code>specr::plot_specs(brms_specs)</code></pre>
<p><img src="into_the_bayesian_multiverse_files/figure-html/unnamed-chunk-13-1.png" width="768" /></p>
</div>
<div id="e.-plot-specification-curve-with-brms-and-lme4-models-together" class="section level2">
<h2>3E. Plot specification curve with <code>brms</code> and <code>lme4</code> models together</h2>
<p>We can also bind the two output data frames with all the specifications we’ve already fit together, then plot a curve across all of them to more directly compare the models. Now, in the <code>models</code> section of <strong>Panel B</strong> in the plot, we can tell whether the model used for each specification was the <code>lme4</code> one (<code>lmer_model_with_motion</code>) or the <code>brms</code> one (<code>brms_model_with_motion</code>)</p>
<pre class="r"><code># columns are slightly different depending on the model type, so use rbind.fill() to deal with this
all_specs = plyr::rbind.fill(specs, brms_specs)
specr::plot_specs(all_specs)</code></pre>
<p><img src="into_the_bayesian_multiverse_files/figure-html/unnamed-chunk-14-1.png" width="768" /></p>
</div>
</div>
<div id="bayesian-specification-curves-using-custom-code" class="section level1">
<h1>4. Bayesian specification curves using custom code</h1>
<p><code>specr</code> is a super useful tool, but when we were first running specification curve analyses, the package hadn’t been released yet. For the analyses in the <a href="https://www.biorxiv.org/content/10.1101/2021.10.08.463601v1">manuscript</a>, we used mostly functions from the <a href="https://www.tidyverse.org/">tidyverse</a>, especially the <a href="https://purrr.tidyverse.org/">purrr</a> package for dealing with nested data, for estimation of many Bayesian models within multiverse analyses (in fact, <code>specr</code> is built using <code>purrr</code>). For more background on extreme <code>tidyverse</code> usage and <code>purrr</code>, check out <a href="https://www.monicathieu.com/posts/2020-04-08-tidy-multilevel/">this walkthrough from Monica Thieu</a>.</p>
<p>While the methods we used required writing more lines of code, one very helpful thing that <code>specr</code> doesn’t do automatically is do is to <em>save the fitted model object for each specification</em>. Storing the fitted model objects will allow us to do a few useful things:</p>
<ol style="list-style-type: decimal">
<li>Extract <a href="https://www.rdocumentation.org/packages/brms/versions/2.9.0/topics/fitted.brmsfit">fitted model predictions</a> (i.e. ‘predicting the regression line’) for each model within a specification curve. In this case, we might want to be able to plot each model’s predictions for amygdala reactivity as a function of age.</li>
<li>Make specification curves for different parameters within the models, without having to rerun them all. For example, here we’ll look at the regression <em>intercept term</em> in a specification curve</li>
<li>Evaluate models more generally, including using posterior predictive checks](<a href="http://paul-buerkner.github.io/brms/reference/pp_check.brmsfit.html" class="uri">http://paul-buerkner.github.io/brms/reference/pp_check.brmsfit.html</a>), or getting <a href="https://paul-buerkner.github.io/brms/reference/loo.brmsfit.html">cross-validated model performance metrics</a> for individual models</li>
</ol>
<div id="a.-convert-the-data-to-long-format" class="section level2">
<h2>4A. Convert the data to long format</h2>
<p>First, we’ll create a grand mean-centered version of our <code>age</code> variable called <code>age_center</code>. Now, a 0 for <code>age_center</code> represents the grand mean age in the dataset, or an age of 12.251. We’ll use this centered age variable for modeling.</p>
<p>Then, we use <a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">tidyr::pivot_longer()</a> to reshape the data into long form, such that all amygdala reactivity estimates are in 1 column called <code>amygdala_reactivity</code>, with a variable called <code>amygdala_measure</code> indicating which amygdala reactivity measure it is.</p>
<pre class="r"><code>fake_data_long = fake_data %&gt;%
  dplyr::mutate(age_center = age - mean(age)) %&gt;%
  tidyr::pivot_longer(contains(&#39;amyg&#39;), names_to = &#39;amygdala_measure&#39;, values_to = &#39;amygdala_reactivity&#39;) 

head(fake_data_long)</code></pre>
<pre><code>## # A tibble: 6 x 11
##      id  wave   age block motion scanner prev_studied grp         age_center
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;
## 1     1     3  16.1     1  0.293       2           NA not_studied       3.87
## 2     1     3  16.1     1  0.293       2           NA not_studied       3.87
## 3     1     3  16.1     1  0.293       2           NA not_studied       3.87
## 4     1     3  16.1     1  0.293       2           NA not_studied       3.87
## 5     1     3  16.1     1  0.293       2           NA not_studied       3.87
## 6     1     3  16.1     1  0.293       2           NA not_studied       3.87
## # … with 2 more variables: amygdala_measure &lt;chr&gt;, amygdala_reactivity &lt;dbl&gt;</code></pre>
</div>
<div id="b.-nest-the-data-within-each-amygdala-measure" class="section level2">
<h2>4B. Nest the data within each amygdala measure</h2>
<p>In this step, we first group the data by <code>amygdala_measure</code>, then use <a href="https://tidyr.tidyverse.org/reference/nest.html">tidyr::nest()</a> to collapse the data within each amygdala measure into a single cell containing a <a href="https://tibble.tidyverse.org/">tibble object</a>(basically a fancy dataframe). Now, our dataset has only 8 rows, with the <code>data</code> column containing an entire dataset for each amygdala measure in each cell. We also add an <code>index</code> variable to keep track of the data shape here.</p>
<pre class="r"><code>fake_data_nested = fake_data_long %&gt;%
  dplyr::group_by(amygdala_measure) %&gt;%
  tidyr::nest() %&gt;%
  dplyr::ungroup() %&gt;%
  dplyr::mutate(., index = 1:nrow(.))

fake_data_nested</code></pre>
<pre><code>## # A tibble: 8 x 3
##   amygdala_measure        data                index
##   &lt;chr&gt;                   &lt;list&gt;              &lt;int&gt;
## 1 ho_right_amyg_beta      &lt;tibble [178 × 10]&gt;     1
## 2 ho_left_amyg_beta       &lt;tibble [178 × 10]&gt;     2
## 3 ho_right_amyg_tstat     &lt;tibble [178 × 10]&gt;     3
## 4 ho_left_amyg_tstat      &lt;tibble [178 × 10]&gt;     4
## 5 native_right_amyg_beta  &lt;tibble [178 × 10]&gt;     5
## 6 native_left_amyg_beta   &lt;tibble [178 × 10]&gt;     6
## 7 native_right_amyg_tstat &lt;tibble [178 × 10]&gt;     7
## 8 native_left_amyg_tstat  &lt;tibble [178 × 10]&gt;     8</code></pre>
</div>
<div id="c.-run-all-model-specifications" class="section level2">
<h2>4C. Run all model specifications!</h2>
<p>To keep things a little more concise for the sake of this walkthrough, we’ll pare down to a total of 16 specifications for this analysis: 8 amygdala measures (right vs. left, native space vs. Harvard-Oxford, beta vs. tstat) X 2 model types (with participant-varying slopes + intercepts vs. only participant-varying intercepts)</p>
<p>Now, we’ll use the <code>purrr:map()</code> function inside <code>dplyr::mutate()</code>. This is a lot at once, but it allows us to make new columns with <code>brms</code> model objects stored in each cell. We use <code>data</code> as the first argument to the <code>purrr:map()</code> function to tell <code>brms::brm()</code> to fit the model to the <em>nested dataset</em> stored in each cell of the `data column.</p>
<p>Since we are making 2 new columns with <code>dplyr::mutate()</code>, one for each of the two model formulations (<code>model_varying_slopes</code>, and <code>model_varying_intercepts</code>), we’ll use <code>tidyr::pivot_longer()</code> again to put all models objects into one <code>model_object</code> column, with the <code>model_type</code> column now indicating whether there are varying slopes or just intercepts. <strong>Note:</strong> this step takes about 15 min (on a 2019 Macbook Pro) because we are running 16 Bayesian models in a row!</p>
<pre class="r"><code>nested_model_frame = fake_data_nested %&gt;%
    dplyr::group_by(amygdala_measure) %&gt;%
    dplyr::mutate(., model_varying_slopes = purrr::map(data, ~brms::brm(amygdala_reactivity ~ age_center + motion + (age|id), 
                                                  data = ., cores = 4, chains = 4)),
                    model_varying_intercepts = purrr::map(data, ~brms::brm(amygdala_reactivity ~ age_center + motion + (1|id), 
                                                  data = ., cores = 4, chains = 4))) %&gt;%
    tidyr::pivot_longer(contains(&#39;model&#39;), names_to = &#39;model_type&#39;, values_to = &#39;model_object&#39;)</code></pre>
</div>
<div id="d.-pull-coefficients-for-each-model-using-broom.mixedtidy" class="section level2">
<h2>4D. Pull coefficients for each model using <code>broom.mixed::tidy()</code></h2>
<p>Now that we’ve run lots of models, we can use <code>broom.mixed::tidy()</code> to pull out all the coefficients for each one. This function is just like the <a href="https://cran.r-project.org/web/packages/broom/vignettes/broom.html">broom::tidy()</a> function, except that it allows us to work with multilevel models (<code>lme4</code>, <code>brms</code>, or <code>rstanarm</code>, for example).</p>
<p>So, well make a new dataframe called <code>purrr_specs</code> where we pull the coefficients for each model (again using <code>purrr::map()</code> for storing the output in individual cells). Finally, we’ll use <a href="https://tidyr.tidyverse.org/reference/nest.html">tidyr::unnest()</a> to ‘flatten’ the coefficient output for each model back into regular dataframe columns. In other words, we’re taking all of the coefficient information, and making 1 column for each piece of information.</p>
<pre class="r"><code>purrr_specs = nested_model_frame %&gt;%
  mutate(., coefs = purrr::map(model_object, ~broom.mixed::tidy(.))) %&gt;%
  dplyr::select(., -data, -model_object) %&gt;%
  tidyr::unnest(coefs)</code></pre>
<p>The output looks very similar to what we previously were working with from <code>specr</code> EXCEPT that we have multiple rows for each specification here. In fact, we have 1 row for each of the coefficients in each model pulled by <code>broom.mixed::tidy()</code> (we are pulling the <code>Intercept</code>, <code>age_center</code>, <code>motion</code> fixed effects terms from the model, parameter estimates for some of the variances/covariances of the participant-varying terms).</p>
<pre class="r"><code>purrr_specs</code></pre>
<pre><code>## # A tibble: 96 x 11
## # Groups:   amygdala_measure [8]
##    amygdala_measure  index model_type    effect component group term    estimate
##    &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;
##  1 ho_right_amyg_be…     1 model_varyin… fixed  cond      &lt;NA&gt;  (Inter…   0.597 
##  2 ho_right_amyg_be…     1 model_varyin… fixed  cond      &lt;NA&gt;  age_ce…  -0.0412
##  3 ho_right_amyg_be…     1 model_varyin… fixed  cond      &lt;NA&gt;  motion   -0.298 
##  4 ho_right_amyg_be…     1 model_varyin… ran_p… cond      id    sd__(I…   0.168 
##  5 ho_right_amyg_be…     1 model_varyin… ran_p… cond      id    sd__age   0.0127
##  6 ho_right_amyg_be…     1 model_varyin… ran_p… cond      id    cor__(…  -0.330 
##  7 ho_right_amyg_be…     1 model_varyin… ran_p… cond      Resi… sd__Ob…   0.741 
##  8 ho_right_amyg_be…     1 model_varyin… fixed  cond      &lt;NA&gt;  (Inter…   0.593 
##  9 ho_right_amyg_be…     1 model_varyin… fixed  cond      &lt;NA&gt;  age_ce…  -0.0419
## 10 ho_right_amyg_be…     1 model_varyin… fixed  cond      &lt;NA&gt;  motion   -0.299 
## # … with 86 more rows, and 3 more variables: std.error &lt;dbl&gt;, conf.low &lt;dbl&gt;,
## #   conf.high &lt;dbl&gt;</code></pre>
<p>If we want to make this look almost like the <code>specr</code> output, we can filter for the model term we’re looking for. In this case, <code>age_center</code> is the parameter of primary interest for looking at age-related change in amygdala reactivity for the fear &gt; baseline contrast.</p>
<pre class="r"><code>purrr_specs_age = dplyr::filter(purrr_specs, term == &#39;age_center&#39;)
purrr_specs_age</code></pre>
<pre><code>## # A tibble: 16 x 11
## # Groups:   amygdala_measure [8]
##    amygdala_measure   index model_type    effect component group term   estimate
##    &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;
##  1 ho_right_amyg_beta     1 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0412
##  2 ho_right_amyg_beta     1 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0419
##  3 ho_left_amyg_beta      2 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0417
##  4 ho_left_amyg_beta      2 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0423
##  5 ho_right_amyg_tst…     3 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0422
##  6 ho_right_amyg_tst…     3 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0431
##  7 ho_left_amyg_tstat     4 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0323
##  8 ho_left_amyg_tstat     4 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0321
##  9 native_right_amyg…     5 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0297
## 10 native_right_amyg…     5 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0311
## 11 native_left_amyg_…     6 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0301
## 12 native_left_amyg_…     6 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0311
## 13 native_right_amyg…     7 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0304
## 14 native_right_amyg…     7 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0319
## 15 native_left_amyg_…     8 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0260
## 16 native_left_amyg_…     8 model_varyin… fixed  cond      &lt;NA&gt;  age_c…  -0.0275
## # … with 3 more variables: std.error &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;</code></pre>
</div>
<div id="e.-custom-sca-plot-function" class="section level2">
<h2>4E. Custom SCA Plot Function</h2>
<p>Now, we want to make our descriptive specification curve plot! We’ve created a function for this study that is <em>very heavily inspired</em> by <a href="https://dcosme.github.io/specification-curves/SCA_tutorial_inferential#3_Plot_specification_curve">Dani Cosme’s spec curve R code</a>.</p>
<p>It’s a chunky function with a lot of code, but overall what it does is this:</p>
<ul>
<li>Make <code>sca_decision_frame</code>, a dataframe with 1 column for each design point coded as a binary decision, such that there is a <code>|</code> to indicate a ‘yes’ and <code>NA</code> to indicate a ‘no’ (this is important for the bottom panel). This dataframe also creates a <code>rank</code> variable to rank specifications by their point estimates, and an <code>overlap_0</code> variable to indicate the sign of the point estimate, plus whether the 95% posterior interval overlaps 0</li>
<li>Convert <code>sca_decision_frame</code> to long format, then group decision points into categories for visualization</li>
<li>For the top panel, plot <code>sca_decision_frame</code> using <a href="https://ggplot2.tidyverse.org/">ggplot2</a> with the <code>rank</code> for each specification on the x-axis and each specification’s point estimate + 95% posterior interval on the y axis. We also color each specification according to the sign of the point estimate and whether the 95% posterior interval overlaps 0. Also, add a horizontal dashed line and an additional point/error bar in black to indicate the median estimate across specifications.</li>
<li>For the bottom panel, plot <code>sca_decision_frame_long</code> using <a href="https://ggplot2.tidyverse.org/">ggplot2</a> as well, with specification <code>rank</code> again on the x-axis. This time, the y axis is categorical, with each level representing a different decision point. Then, we use <code>geom_text(label=choice)</code>, where <code>choice</code> is the variable coded as either <code>|</code> or <code>NA</code> for each binarized decision point. This will give us a little vertical tick on the plot for each row every time the <code>choice</code> was ‘yes’, and a blank space every time the <code>choice</code> was ‘no’. So, we’ll get 1 row each on the y-axis for each binarized choice, then vertical ticks marking if that choice was made or not. For each choice, we’ll also plot a black point + horizontal interval representing the median <code>rank</code> and 25%-75% quantiles of the <code>rank</code> when the <code>choice</code> was ‘yes’.</li>
<li>Lastly, we use <a href="https://wilkelab.org/cowplot/articles/plot_grid.html">cowplot::plot_grid()</a> to vertically align the two panels and label them. Then, we return the data frames &amp; plot objects in a named list.</li>
</ul>
<p><strong>Note:</strong> this is a slightly shortened version of the function intended specifically for amygdala reactivity analyses in the current study. It will have to be somewhat modified to work more generally in other contexts.</p>
<pre class="r fold-hide"><code>make_sca_plot = function(specs, fork_list, plot_title, y_label, term_choice){
  # compile sca decision frame
  sca_decision_frame = specs %&gt;%
    ungroup %&gt;% 
    dplyr::filter(term == term_choice) %&gt;%
    dplyr::arrange(estimate) %&gt;%
    mutate(., rank = 1:nrow(.),
           tstat = ifelse(grepl(&#39;tstat&#39;, amygdala_measure), &#39;|&#39;, NA),
           random_slopes = ifelse(grepl(&#39;intercepts&#39;, model_type), &#39;|&#39;, NA),
           amyg_right = ifelse(grepl(&#39;right&#39;, tolower(amygdala_measure)), &#39;|&#39;, NA),
           amyg_left = ifelse(grepl(&#39;left&#39;, tolower(amygdala_measure)), &#39;|&#39;, NA),
           native_space = ifelse(grepl(&#39;native&#39;, amygdala_measure), &#39;|&#39;, NA),
           overlap_0 = case_when(
             conf.low &lt; 0 &amp; conf.high &lt; 0 ~ &#39;neg_y&#39;,
             conf.low &lt; 0 &amp; estimate &lt; 0 &amp; conf.high &gt; 0 ~ &#39;neg_n&#39;,
             conf.low &lt; 0 &amp; estimate &gt; 0 &amp; conf.high &gt; 0 ~ &#39;pos_n&#39;,
             conf.low  &gt; 0 &amp; conf.high &gt; 0 ~ &#39;pos_y&#39;,
           ))

  # median model
  median_model_frame = sca_decision_frame %&gt;%
    summarise(estimate = median(estimate), 
              conf.low = median(conf.low), conf.high = median(conf.high), 
              rank= median(rank))
  
  # order factor levels for overlap_0 variable
  sca_decision_frame$overlap_0 = factor(sca_decision_frame$overlap_0, 
                                        levels = c(&quot;neg_y&quot;, &quot;neg_n&quot;, &quot;pos_n&quot;, &quot;pos_y&quot;))
  
  # convert to long, assign decition types
  sca_decision_frame_long = sca_decision_frame %&gt;%
    tidyr::pivot_longer(names_to = &#39;fork&#39;, values_to = &#39;choice&#39;, all_of(fork_list)) %&gt;%
    mutate(decisiontype = case_when(
      grepl(&#39;amyg&#39;, fork) ~ &#39;Amygdala\n Roi&#39;,
      fork == &#39;native_space&#39; ~ &#39;Amygdala\n Roi&#39;,
      fork %in% c(&#39;random_slopes&#39;, &#39;model_type&#39;) ~ &#39;Group-Level\nModel&#39;,
      fork %in% c(&#39;tstat&#39;) ~ &#39;Subject-Level\nModel&#39;
    ))
  
  # get average rank of each amygdala_measure by beta estimate
  sca_decision_frame_long_ranks = sca_decision_frame_long %&gt;%
    dplyr::filter(choice == &#39;|&#39;) %&gt;%
    dplyr::group_by(fork) %&gt;%
    summarise(mean_rank = -1*mean(rank))
  
  # join ranks with decision frame
  sca_decision_frame_long = left_join(sca_decision_frame_long, sca_decision_frame_long_ranks)
  
  # rename variables to be human-interpretable
  sca_decision_frame_long$fork = 
    dplyr::recode(sca_decision_frame_long$fork, 
                  &#39;tstat&#39; = &#39;use tstats (vs. beta estimates)&#39;,
                  &#39;native_space&#39; = &#39;native space (vs. mni space)&#39;,
                  &#39;model_type&#39; = &#39;random intercepts only (vs. random slopes)&#39;,
                  &#39;amyg_right&#39; = &#39;right amygdala&#39;,
                  &#39;amyg_left&#39; =  &#39;left amygdala&#39;)
  
  # reorder forks by mean rank
  sca_decision_frame_long$fork_ordered = reorder(sca_decision_frame_long$fork,  
                                                 sca_decision_frame_long$mean_rank)
  
  # color palette to code the following:
  # blue = negative, distinct from 0
  # red = negative, not distinct from 0
  # green = positive, not distinct from 0
  # purple = positive, distinct from 0
  if(&#39;neg_y&#39; %in% sca_decision_frame$overlap_0){
    my_colors &lt;- RColorBrewer::brewer.pal(4, &quot;Set1&quot;)[1:4]
  }else if(&#39;neg_n&#39; %in% sca_decision_frame$overlap_0){
    my_colors &lt;- RColorBrewer::brewer.pal(4, &quot;Set1&quot;)[2:4]
  }else if(&#39;pos_n&#39; %in% sca_decision_frame$overlap_0){
    my_colors &lt;- RColorBrewer::brewer.pal(4, &quot;Set1&quot;)[3:4]
  }else{
    my_colors &lt;- RColorBrewer::brewer.pal(4, &quot;Set1&quot;)[4:4]
  }
  
  # recode overlap 0 markings for informative legend
  sca_decision_frame$overlap_0 = dplyr::recode(sca_decision_frame$overlap_0,
                                               &#39;neg_y&#39; = &#39;-, 95% PI excluding 0&#39;,
                                               &#39;neg_n&#39; = &#39;-, 95% PI including 0&#39;,
                                               &#39;pos_n&#39; = &#39;+, 95% PI including 0&#39;,
                                               &#39;pos_y&#39; = &#39;+, 95% PI excluding 0&#39;)
  
  
  # summary for lower plot (median + IQR)
  decision_summary = sca_decision_frame_long %&gt;% 
    group_by(decisiontype, choice, fork, fork_ordered) %&gt;%
    summarise(n = n(), median_rank = median(rank), 
              lwr_rank = quantile(rank, .25), 
              upr_rank = quantile(rank, .75)) %&gt;%
    dplyr::filter(choice ==&#39;|&#39;)
  
  # top panel
  sca_top = 
    ggplot(sca_decision_frame, aes(x = rank, y = estimate, color = overlap_0)) +
    geom_hline(yintercept = 0, lty = 1, color = &#39;black&#39;) +
    geom_hline(yintercept = median(sca_decision_frame$estimate), color = &#39;black&#39;, lty = 2) +
    geom_point(alpha = .5) + 
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0, lwd = .15, alpha = .5) +
    geom_point(data = median_model_frame,aes(x = rank, y = estimate), 
               color = &#39;black&#39;) +
    geom_errorbar(data = median_model_frame, 
                  aes(x = rank, y = estimate, ymin = conf.low, ymax = conf.high), 
                  color = &#39;black&#39;, width = 0) +
    labs(x = &#39;&#39;, y = y_label, title = plot_title) +
    theme_classic() +
    theme(legend.position = &#39;top&#39;, legend.title = element_blank()) +
    scale_color_manual(values = my_colors)
  
  # lower panel
  sca_bottom = 
    ggplot(sca_decision_frame_long, aes(x = rank, y = fork_ordered, color = overlap_0)) +
    geom_text(aes(label = choice), alpha = .4) +
    facet_grid(rows = vars(decisiontype), drop = TRUE, 
               scales = &#39;free_y&#39;, space = &#39;free_y&#39;) +
    geom_point(data = decision_summary, aes(x = median_rank, y = fork_ordered), 
               color = &#39;black&#39;) +
    geom_errorbarh(data = decision_summary, 
                   aes(x = median_rank, y = fork_ordered, xmin = lwr_rank, xmax = upr_rank), 
                   color = &#39;black&#39;, width = 0) +
    labs(x = &quot;Analysis specifications ranked by beta estimates&quot;, y = &quot;Decision Points&quot;) + 
    theme_bw() + 
    theme(legend.title = element_text(size = 10),
          legend.text = element_text(size = 8),
          axis.text = element_text(color = &quot;black&quot;, size = 8),
          legend.position = &quot;none&quot;,
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.background = element_blank(),
          strip.text.y = element_text(size = 8)) +
    scale_color_manual(values = my_colors) 
    
  
  # put together using cowplot
  sca_panel = cowplot::plot_grid(sca_top, sca_bottom, ncol = 1, align = &quot;v&quot;, axis = &#39;lr&#39;, 
                                 labels = c(&#39;A&#39;, &#39;B&#39;))
  
  # return a list including formatted data, as well as plots
  return(list(&#39;sca_decision_frame&#39; = sca_decision_frame, 
              &#39;sca_decision_frame_long&#39; = sca_decision_frame_long,
              &#39;sca_top&#39; = sca_top,
              &#39;sca_bottom&#39; = sca_bottom,
              &#39;sca_panel&#39; = sca_panel))
}</code></pre>
</div>
<div id="f.-plot-custom-brms-spec-curve" class="section level2">
<h2>4F. Plot custom <code>brms</code> spec curve</h2>
<p>First, we run <code>make_sca_plot()</code> to generate the plot object</p>
<pre class="r"><code>age_center_spec_curve = make_sca_plot(specs = purrr_specs, 
                  fork_list = c(&#39;tstat&#39;, &#39;random_slopes&#39;, &#39;amyg_right&#39;, &#39;amyg_left&#39;, &#39;native_space&#39;), 
                  y_label = &#39;Age-related change in amyg reactivity\nFear &gt; Neutral Faces&#39;, 
                  term_choice = &#39;age_center&#39;, 
                  plot_title = &#39;Custom plot for simulated data spec curve&#39;)</code></pre>
<p>Then display the <code>sca_panel</code>. Although this is a slightly different set of specifications to the previous curves, we can see that the results are pretty similar – most specifications show negative age-related change estimates.</p>
<pre class="r"><code>age_center_spec_curve$sca_panel</code></pre>
<p><img src="into_the_bayesian_multiverse_files/figure-html/unnamed-chunk-23-1.png" width="768" /></p>
</div>
<div id="g.-a-spec-curve-for-the-model-intercept" class="section level2">
<h2>4G. A spec curve for the model intercept</h2>
<p>Here, we can also plot a specification curve for the regression <em>intercept</em> term. In this case, the intercept represents the average estimated amygdala reactivity to fear faces &gt; baseline for a participant of <code>age_center=0</code> (age ~12.3 years) and <code>motion=0</code> (average head motion amounts). We can do this by setting <code>term_choice = '(Intercept)'</code> and re-running <code>make_sca_plot()</code></p>
<pre class="r"><code>intercept_spec = make_sca_plot(specs = purrr_specs, 
                    fork_list = c(&#39;tstat&#39;, &#39;random_slopes&#39;, &#39;amyg_right&#39;, &#39;amyg_left&#39;, &#39;native_space&#39;), 
                    y_label = &#39;Estimated Amygdala Reactivity at Age 12.3\nFear &gt; Neutral Faces&#39;, 
                    term_choice = &#39;(Intercept)&#39;, 
                    plot_title = &#39;Custom plot for the spec curve for the model *intercepts*&#39;)</code></pre>
<p>This plot shows that all specifications estimate positive average amygdala reactivity a participant at age 12.3 and mean head motion levels, with 95% posterior intervals excluding 0.</p>
<pre class="r"><code>intercept_spec$sca_panel</code></pre>
<p><img src="into_the_bayesian_multiverse_files/figure-html/unnamed-chunk-25-1.png" width="768" /></p>
</div>
</div>
<div id="get-fitted-model-predictions-from-the-custom-brms-multiverse" class="section level1">
<h1>5. Get fitted model predictions from the custom <code>brms</code> multiverse</h1>
<p>Because we’ve saved the fitted model objects into a nested data frame for each specification in our custom multiverse, we can also <em>extract fitted predictions</em> for each model. Here, we’ll do this to visualize each model’s predictions for average amygdala reactivity (for fear faces &gt; baselines) as a function of age.</p>
<p>First, we’ll define a grid of predictor values for which to make predictions. Here, we’ll set <code>motion=0</code> (average head motion across the dataset), and <code>age_center=-8:10</code> to be able to generate a prediction for a range of ages</p>
<pre class="r"><code>pred_grid = expand.grid(age_center = -8:10, motion = 0) %&gt;%
  dplyr::mutate(age = age_center + mean(fake_data$age))

head(pred_grid)</code></pre>
<pre><code>##   age_center motion      age
## 1         -8      0 4.251169
## 2         -7      0 5.251169
## 3         -6      0 6.251169
## 4         -5      0 7.251169
## 5         -4      0 8.251169
## 6         -3      0 9.251169</code></pre>
<p>Now, we can use <code>purrr::map()</code> again with <a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/fitted">stats::fitted()</a> to generate fitted values (estimates of the <a href="https://mc-stan.org/rstanarm/reference/posterior_linpred.stanreg.html">linear predictor</a>) for each specification.</p>
<ul>
<li>We’ll use the arguments <code>newdata = pred_grid</code> to specify which values of the predictors to generate predictions for, and <code>re_formula = NA</code> to indicate not to worry about participant-varying random effects here</li>
<li>Just like when we pulled the model coefficients with <code>broom.mixed::tidy()</code>, the predictions for each specification are nested into cells of the <code>model_preds</code> column. We use <code>tidyr::unnest(model_preds)</code> to flatten them back out and get a data frame (in long form) of predictions for each model</li>
</ul>
<pre class="r"><code>purrr_specs_preds = nested_model_frame %&gt;%
  dplyr::mutate(model_preds = purrr::map(model_object, ~stats::fitted(., newdata = pred_grid, re_formula = NA) %&gt;%
               cbind(pred_grid, .))) %&gt;%
  dplyr::select(-data, -model_object) %&gt;%
  tidyr::unnest(model_preds)</code></pre>
<p>For each specification, and for each level of <code>age_center</code>, we get an <code>Estimate</code> (the median posterior estimate), as well as the upper/lower bounds of the 95% posterior interval.</p>
<pre class="r"><code>head(purrr_specs_preds)</code></pre>
<pre><code>## # A tibble: 6 x 10
## # Groups:   amygdala_measure [1]
##   amygdala_measure  index model_type  age_center motion   age Estimate Est.Error
##   &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;            &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 ho_right_amyg_be…     1 model_vary…         -8      0  4.25    0.926    0.123 
## 2 ho_right_amyg_be…     1 model_vary…         -7      0  5.25    0.885    0.111 
## 3 ho_right_amyg_be…     1 model_vary…         -6      0  6.25    0.844    0.0987
## 4 ho_right_amyg_be…     1 model_vary…         -5      0  7.25    0.803    0.0875
## 5 ho_right_amyg_be…     1 model_vary…         -4      0  8.25    0.762    0.0774
## 6 ho_right_amyg_be…     1 model_vary…         -3      0  9.25    0.720    0.0688
## # … with 2 more variables: Q2.5 &lt;dbl&gt;, Q97.5 &lt;dbl&gt;</code></pre>
<div id="a.-spaghetti-plot-of-model-predictions-across-specifications" class="section level2">
<h2>5A. Spaghetti plot of model predictions across specifications</h2>
<p>Now that we’ve extracted the fitted predictions for each model, we can plot the group-average ‘regression line’ for each one.</p>
<ul>
<li>Here we’ll color the specifications by whether they included participant-varying slopes, or just intercepts.</li>
<li>Here, each line represents the model predictions for 1 specification.</li>
<li>This can be a good way to summarize fitted model predictions across a multiverse, even if we can’t see the uncertainty about the regression line for any given specification.</li>
</ul>
<pre class="r"><code>ggplot(data = purrr_specs_preds, aes(x = age, y = Estimate, 
                                     group = interaction(amygdala_measure, model_type), 
                                     color = model_type)) +
  geom_hline(yintercept = 0, lty = 2) +
  geom_line() +
  theme_bw() +
  labs(y = &#39;Estimated Amygdala Reactivity\nFear &gt; Baseline&#39;)</code></pre>
<p><img src="into_the_bayesian_multiverse_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
</div>
<div id="b.-panel-plot-of-fitted-predictions" class="section level2">
<h2>5B. Panel plot of fitted predictions</h2>
<p>We can also plot each specification here as a separate panel, and include the 95% posterior interval for the fitted model predictions at each level of <code>age</code>. This is just about possible with 16 specifications, but many more than that and we probably won’t have room on the screen to view them all.</p>
<pre class="r"><code>ggplot(data = purrr_specs_preds, aes(x = age, y = Estimate)) +
  geom_hline(yintercept = 0, lty = 2) +
  geom_line() +
  geom_ribbon(aes(ymin = `Q2.5`, ymax = `Q97.5`), alpha = .3) + 
  theme_bw() +
  labs(y = &#39;Estimated Amygdala Reactivity\nFear &gt; Baseline&#39;, 
       title = &#39;Panel plot with fitted model predictions for each specification&#39;) +
  facet_grid(rows = vars(model_type), cols = vars(amygdala_measure)) +
  theme(strip.text = element_text(size = 6))</code></pre>
<p><img src="into_the_bayesian_multiverse_files/figure-html/unnamed-chunk-30-1.png" width="864" /></p>
</div>
<div id="c.-plot-an-individual-models-fitted-predictions-with-raw-data" class="section level2">
<h2>5C. Plot an individual model’s fitted predictions with raw data</h2>
<p>As a check of a model’s fit, it is often helpful to look at the fitted predictions overplotted on top of the raw data points. We can do that by filtering our predictions to just look at one single specification (<code>amygdala_measure == 'ho_right_amyg_beta'</code>, <code>model_type == 'model_varying_slopes'</code>), then plotting with <code>ggplot()</code> with age on the x-axis and estimated amygdala-reactivity on the y-axis, along with the raw data used by that specification.</p>
<pre class="r"><code>one_spec_preds = dplyr::filter(purrr_specs_preds, amygdala_measure == &#39;ho_right_amyg_beta&#39;, model_type == &#39;model_varying_slopes&#39;)

ggplot(fake_data, aes(x = age, y = ho_right_amyg_beta)) +
  geom_hline(yintercept = 0, lty = 2) +
  geom_point() + 
  geom_line(aes(group = id), alpha = .1) +
  geom_line(data = one_spec_preds, aes(x = age, y = Estimate)) +
  geom_ribbon(data = one_spec_preds, aes(x = age, y = Estimate, ymin = `Q2.5`, ymax = `Q97.5`), alpha = .3) +
  theme_bw() +
  labs(y = &#39;Estimated Amygdala Reactivity\nFear &gt; Baseline&#39;, 
       title = &#39;Model predictions &amp; raw data for one specification&#39;, 
       subtitle = &#39;Harvard-Oxford right amygdala betas, brms model with varying slopes&#39;)</code></pre>
<p><img src="into_the_bayesian_multiverse_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>This is a nice check of our model fits! However, the more specifications we have, the more difficult it is to look at so many plots for individual specifications like this. To that end, for this study we have made <a href="https://pbloom.shinyapps.io/amygdala_mpfc_multiverse/">an interactive multiverse explorer Shiny app</a> for browsing model fits across specifications.</p>
</div>
</div>
<div id="multiverse-model-checks" class="section level1">
<h1>6. Multiverse model checks</h1>
<div id="a.-graphical-posterior-predictive-checks" class="section level2">
<h2>6A. Graphical posterior predictive checks</h2>
<p>In addition to looking at the coefficients and predictions from our different specifications, we may also want to perform <a href="https://cran.r-project.org/web/packages/LaplacesDemon/vignettes/BayesianInference.pdf">posterior predictive checks</a> for each one.</p>
<p>What is a posterior predictive check? * We draw a “simulated dataset” from the model’s posterior distribution, then compare the overall distribution to the real observed data (see <a href="https://arxiv.org/abs/1709.01449">Gabry et al., 2019</a>). * Basically, a posterior predictive check is helpful in answering the question “is the model giving us a distribution of predictions that matches the real data”. If the distributions match, this doesn’t mean the model is <em>good</em> per se (the individual predictions could still be very bad), but if the distributions <em>don’t match</em>, then we know that the model is definitely not doing a good job at describing the data generation process.</p>
<p>To get these graphical summaries of the posterior predictive check, we use the <a href="http://paul-buerkner.github.io/brms/reference/pp_check.brmsfit.html">pp_check()</a> function for <code>brms</code> models (I know…). Here, we’ll first combine this with <code>purrr_map()</code> to draw 10 samples from the posterior predictive distribution for all observations in the real dataset.</p>
<pre class="r"><code>pp_check_grid = nested_model_frame %&gt;%
  dplyr::mutate(chex = purrr::map(model_object, ~brms::pp_check(., nsamples = 10)))

# Make a new column to label all specifications
nested_model_frame = mutate(nested_model_frame,
                            spec = paste0(amygdala_measure, &#39;, &#39;, model_type))</code></pre>
<p>Now, we can use <code>cowplot::plot_grid()</code> again to make a panel of all 16 graphical posterior predictive checks, one for each model in our specification curve. We can use the <code>labels</code> argument to label each plot for each specification. Here, <span class="math inline">\(y\)</span> represents the distribution of actual observations, and each <span class="math inline">\(y_{rep}\)</span> distribution is one set of the posterior draws.</p>
<pre class="r"><code>cowplot::plot_grid(plotlist = pp_check_grid$chex, 
                   labels = as.vector(nested_model_frame$spec), 
                   label_size = 4, label_x = 0, hjust = 0)</code></pre>
<p><img src="into_the_bayesian_multiverse_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>Again, this is a more descriptive analysis – it allows us to visually compare which specifications result in a model posterior distribution that most closely matches the observed data.</p>
</div>
<div id="b.-approximate-leave-one-out-loo-cross-validation" class="section level2">
<h2>6B. Approximate leave-one-out (LOO) cross-validation</h2>
<p>We might often want to to get a specification curve for the relative model ‘goodness-of-fit’, or an approximate of predictive performance, for all specifications. We can use <a href="https://www.rdocumentation.org/packages/brms/versions/2.15.0/topics/loo.brmsfit">brms::loo()</a> to perform approximate leave-one-out (LOO) cross-validation for each model. So, here, well use <code>purrr::map()</code> again with <code>brms:loo()</code> to get these estimates</p>
<ul>
<li>This function will give us a cross-validated estimate for the expected log pointwise predictive density, or <a href="https://avehtari.github.io/modelselection/CV-FAQ.html">ELPD</a>, as a metric for the “expected predictive performance” for a model. ELPD as estimated by LOO cross-validation is similar in purpose to the Akaike Information Criterion (AIC), although it takes model priors into account and does not make distributional assumptions about the posterior (see <a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">Vehtari, Gelman, &amp; Gabry, 2017</a>)</li>
<li>This function uses Pareto smoothed importance sampling (<a href="https://arxiv.org/abs/1507.02646">Vehtari et al., 2015</a>) for cross-validation here, which means that unless results are very bad, we won’t have to repeatedly refit the Bayesian models (which would take a while)</li>
</ul>
<pre class="r"><code>loo_cv_grid = nested_model_frame %&gt;%
  dplyr::mutate(loo_cv = purrr::map(model_object, ~brms::loo(.)$estimates))</code></pre>
<p>Now, we can unnest our ELPD estimates as well as their standard errors, then plot them for each specification.</p>
<pre class="r"><code># unnest and wrangle elpd estimates into the data frame 
loo_unnest = unnest(loo_cv_grid, loo_cv)
loo_cv_grid$elpd_loo = as.numeric(loo_unnest$loo_cv[,1][row.names(loo_unnest$loo_cv)==&#39;elpd_loo&#39;])
loo_cv_grid$elpd_loo_se = as.numeric(loo_unnest$loo_cv[,2][row.names(loo_unnest$loo_cv)==&#39;elpd_loo&#39;])</code></pre>
<p>The raw value of ELPD by itself doesn’t mean much intrinsically, but since higher (or less negative) ELPD is better, we can use this to compare expected model predictive performance. So, we can see here that model predictive performance is mostly impacted here by the amygdala measure used, although for most measures, the models with varying intercepts have slightly higher (better) ELPD than models with varying slopes.</p>
<pre class="r"><code>ggplot(loo_cv_grid, aes(x = fct_reorder(spec, elpd_loo), y = elpd_loo)) +
  geom_point() +
  geom_errorbar(aes(ymin = elpd_loo - elpd_loo_se, ymax = elpd_loo + elpd_loo_se)) +
  coord_flip() +
  theme_bw() +
  labs(x = &#39;Specification&#39;, y = &#39;elpd_loo&#39;)</code></pre>
<p><img src="into_the_bayesian_multiverse_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>While the values of <code>elpd_loo</code> don’t tell us so much intrinsically, the <em>relative ranking</em> of the values tells us which models (i.e. the ones with larger <code>elpd_loo</code>, towards the right of the plot) have better expected predictive performance.</p>
</div>
</div>
<div id="recap" class="section level1">
<h1>7. Recap</h1>
<p>Though we can’t share the real data used in the current study, hopefully these walkthroughs of multiverse analyses using publicly available <a href="https://github.com/pab2163/amygdala_mpfc_multiverse/blob/master/docs/simulated_amygdala_reactivity.csv">simulated data</a> have been helpful! Overall, we find that specification curves can a super useful as a way to examine the robustness of results when we’re faced with many analysis decision points. For anyone who is interested, good luck <em>traveling the multiverse!</em></p>
<p><strong>Questions?</strong> Feel free to reach out to <a href="mailto:paul.bloom@columbia.edu" class="email">paul.bloom@columbia.edu</a></p>
</div>
<div id="resources" class="section level1">
<h1>Resources</h1>
<div id="readings-on-specification-curves-multiverse-analyses" class="section level2">
<h2>Readings on specification curves &amp; multiverse analyses</h2>
<ul>
<li><a href="https://journals.sagepub.com/doi/10.1177/2515245920954925">Del Guidice &amp; Gangestad, 2021</a></li>
<li>Simonsohn, Simmons, &amp; Nelson <a href="https://sticerd.lse.ac.uk/seminarpapers/psyc16022016.pdf">2015</a> &amp; <a href="https://www.nature.com/articles/s41562-020-0912-z">2020</a></li>
<li><a href="https://journals.sagepub.com/doi/10.1177/1745691616658637">Steegen et al., 2016</a></li>
</ul>
</div>
<div id="a-few-papers-using-multiverse-analyses" class="section level2">
<h2>A few papers using multiverse analyses</h2>
<ul>
<li><a href="https://psyarxiv.com/23mu5/">Cosme &amp; Lopez, 2021</a></li>
<li><a href="https://www.nature.com/articles/s41562-018-0506-1">Orben &amp; Przybylski, 2019</a></li>
<li><a href="https://psycnet.apa.org/record/2020-15046-001">Frey et al., 2021</a></li>
<li><a href="https://www.tandfonline.com/doi/full/10.1080/09658211.2020.1797095">Wessel et al., 2020</a></li>
</ul>
</div>
<div id="resources-for-conducting-specification-curve-analyses-with-r" class="section level2">
<h2>Resources for conducting specification curve analyses with R</h2>
<ul>
<li>The <a href="https://github.com/masurp/specr">specr</a> R package for making specification curves</li>
<li><a href="https://dcosme.github.io/specification-curves/SCA_tutorial_inferential">Dani Cosme’s comprehensive SCA tutorial</a></li>
<li><a href="https://www.monicathieu.com/posts/2020-04-08-tidy-multilevel/">Monica Thieu’s tutorial</a> on wrangling multilevel data using the tidyverse, especially with <code>purrr</code></li>
<li><a href="https://cu-psych-computing.github.io/cu-psych-comp-tutorial/">Columbia Psychology Scientific Computing R tutorials, videos, &amp; challenges</a> (good for getting started with R)</li>
</ul>
</div>
<div id="r-session-info" class="section level2">
<h2>R session info</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.6.2 (2019-12-12)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Mojave 10.14.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] broom.mixed_0.2.4  lme4_1.1-21        Matrix_1.2-18      cowplot_1.0.0     
##  [5] brms_2.12.0        Rcpp_1.0.7         specr_0.2.2        forcats_0.4.0     
##  [9] stringr_1.4.0      dplyr_1.0.3        purrr_0.3.3        readr_1.3.1       
## [13] tidyr_1.0.2        tibble_3.1.2       ggplot2_3.3.3.9000 tidyverse_1.3.0   
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.1.5      plyr_1.8.5          
##   [4] igraph_1.2.4.2       TMB_1.7.16           splines_3.6.2       
##   [7] crosstalk_1.0.0      rstantools_2.0.0     inline_0.3.15       
##  [10] digest_0.6.27        htmltools_0.4.0      viridis_0.5.1       
##  [13] rsconnect_0.8.16     fansi_0.5.0          magrittr_2.0.1      
##  [16] graphlayouts_0.7.1   modelr_0.1.5         RcppParallel_4.4.4  
##  [19] matrixStats_0.55.0   xts_0.12-0           prettyunits_1.1.1   
##  [22] colorspace_2.0-1     rvest_0.3.5          ggrepel_0.8.1       
##  [25] haven_2.2.0          xfun_0.19            callr_3.4.2         
##  [28] crayon_1.4.1         jsonlite_1.6.1       zoo_1.8-7           
##  [31] glue_1.4.2           polyclip_1.10-0      gtable_0.3.0        
##  [34] V8_3.0.1             pkgbuild_1.0.6       rstan_2.21.1        
##  [37] abind_1.4-5          scales_1.1.1         mvtnorm_1.1-0       
##  [40] DBI_1.1.0            miniUI_0.1.1.1       viridisLite_0.4.0   
##  [43] xtable_1.8-4         stats4_3.6.2         StanHeaders_2.21.0-1
##  [46] DT_0.11              htmlwidgets_1.5.1    httr_1.4.1          
##  [49] threejs_0.3.3        RColorBrewer_1.1-2   ellipsis_0.3.2      
##  [52] pkgconfig_2.0.3      loo_2.2.0            farver_2.1.0        
##  [55] dbplyr_1.4.2         utf8_1.2.1           tidyselect_1.1.0    
##  [58] labeling_0.4.2       rlang_0.4.11         reshape2_1.4.3      
##  [61] later_1.0.0          munsell_0.5.0        cellranger_1.1.0    
##  [64] tools_3.6.2          cli_2.5.0            generics_0.0.2      
##  [67] broom_0.7.4          ggridges_0.5.2       evaluate_0.14       
##  [70] fastmap_1.0.1        yaml_2.2.1           processx_3.4.2      
##  [73] knitr_1.30           fs_1.3.1             tidygraph_1.2.0     
##  [76] ggraph_2.0.4         nlme_3.1-142         mime_0.9            
##  [79] xml2_1.2.2           compiler_3.6.2       bayesplot_1.7.1     
##  [82] shinythemes_1.2.0    rstudioapi_0.10      curl_4.3            
##  [85] reprex_0.3.0         tweenr_1.0.1         stringi_1.4.6       
##  [88] ps_1.3.2             Brobdingnag_1.2-6    lattice_0.20-38     
##  [91] nloptr_1.2.1         markdown_1.1         shinyjs_1.1         
##  [94] vctrs_0.3.8          pillar_1.6.1         lifecycle_1.0.0     
##  [97] bridgesampling_1.0-0 httpuv_1.5.2         R6_2.5.0            
## [100] promises_1.1.0       gridExtra_2.3        codetools_0.2-16    
## [103] boot_1.3-23          colourpicker_1.0     MASS_7.3-51.4       
## [106] gtools_3.8.1         assertthat_0.2.1     withr_2.4.2         
## [109] shinystan_2.5.0      parallel_3.6.2       hms_0.5.3           
## [112] grid_3.6.2           coda_0.19-3          minqa_1.2.4         
## [115] rmarkdown_2.1        ggforce_0.3.1        shiny_1.4.0         
## [118] lubridate_1.7.4      base64enc_0.1-3      dygraphs_1.1.1.6</code></pre>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
